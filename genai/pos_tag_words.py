from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import time
import json
from threading import Lock
import random
import pycantonese
import re
import argparse

random.seed(42)

parser = argparse.ArgumentParser(description="POS Tagging with OpenAI API")
parser.add_argument('--model', type=str, choices=['deepseek-chat', 'deepseek-coder', 'gpt-4o', 'gpt-4o-mini', 'qwen-max', 'qwen-plus', 'qwen-turbo'], required=True, help='Model to use for POS tagging')
parser.add_argument('--sample_size', type=int, default=100, help='Number of samples to test')
parser.add_argument('--prompt_version', type=str, choices=['v1', 'v2'], required=True, help='Prompt version to use for POS tagging')
parser.add_argument('--prompt_dataset', type=str, choices=['hkcancor', 'ud_yue'], required=True, help='Dataset to use for POS tagging')
parser.add_argument('--eval_dataset', type=str, choices=['hkcancor', 'ud_yue'], required=True, help='Dataset to use for evaluation')
parser.add_argument('--max_workers', type=int, default=100, help='Maximum number of workers to use for parallel processing')
args = parser.parse_args()

if args.model.startswith('deepseek'):
    base_url = "https://api.deepseek.com"
    with open('deepseek_api_key.txt', 'r') as file:
        api_key = file.read().strip()
elif args.model.startswith('gpt-4o'):
    base_url = None
    with open('openai_api_key.txt', 'r') as file:
        api_key = file.read().strip()
elif args.model.startswith('qwen'):
    base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    with open('qwen_api_key.txt', 'r') as file:
        api_key = file.read().strip()

client = OpenAI(api_key=api_key, base_url=base_url)

valid_pos_tags = {"ADJ", "ADP", "ADV", "AUX", "CCONJ", "DET", "INTJ", "NOUN", "NUM", "PART", "PRON", "PROPN", "PUNCT", "SCONJ", "SYM", "VERB", "X"}

def segment_words(sentence):
    attempts = 0
    while True:
        try:
            response = client.chat.completions.create(
                model=args.model,
                messages=[{
                            "role": "system",
                            "content": pos_prompt
                          },
                          {
                            "role": "user",
                            "content": sentence
                          }],
                response_format={
                    'type': 'json_object'
                },
                max_tokens=2000,
                temperature=1.1,
                stream=False
            )
            result = response.choices[0].message.content
            try:
                result_json = json.loads(result)
            except json.JSONDecodeError as e:
                raise Exception(f"Failed to parse JSON response: {str(e)}")
            if "pos_tagged_words" in result_json and isinstance(result_json["pos_tagged_words"], list) and \
                all(isinstance(item, list) and len(item) == 2 and (all(isinstance(sub_item, str) for sub_item in item)) for item in result_json["pos_tagged_words"]):
                concatenated_words = "".join([word for word, pos in result_json["pos_tagged_words"]])
                if concatenated_words == sentence:
                    for word, pos in result_json["pos_tagged_words"]:
                        if pos not in valid_pos_tags:
                            raise Exception(f"Invalid POS tag '{pos}' in the result")
                    return result_json["pos_tagged_words"]
                else:
                    raise Exception(f"Segmentation result does not match the input sentence")
            else:
                raise Exception(f"Invalid segmentation result format")
        except Exception as e:
            time.sleep(1)
            attempts += 1
            if attempts >= 3:
                return {'error': str(e), 'result': result}


def load_hkcancor():
    # Load HKCanCor data
    hkcancor_data = pycantonese.hkcancor()

    # Gather all word segmented utterances
    utterances = [[(token.word, pycantonese.pos_tagging.hkcancor_to_ud(token.pos)) for token in utterance] for utterance in hkcancor_data.tokens(by_utterances=True)]

    return utterances


def split_hkcancor():
    random.seed(42)

    utterances = load_hkcancor()

    latin_fragmented_utterances = []
    latin_complete_utterances = []
    non_latin_utterances = []

    for utterance in utterances:
        if any(re.search(r'[a-zA-Z0-9]', char) for (word, pos) in utterance for char in word):
            if ''.join(word for word, pos in utterance).count('"') % 2 != 0:
                latin_fragmented_utterances.append(utterance)
            else:
                latin_complete_utterances.append(utterance)
        else:
            non_latin_utterances.append(utterance)

    # Sample 3 random latin_complete_utterances, 3 random latin_fragmented_utterances, and 4 random non_latin_utterances for in_context_samples
    random.shuffle(latin_complete_utterances)
    random.shuffle(latin_fragmented_utterances)
    random.shuffle(non_latin_utterances)
    in_context_samples = latin_complete_utterances[:3] + latin_fragmented_utterances[:3] + non_latin_utterances[:4]
    random.shuffle(in_context_samples)
    
    return in_context_samples


def upos_id_to_str(upos_id):
    names=[
        "NOUN",
        "PUNCT",
        "ADP",
        "NUM",
        "SYM",
        "SCONJ",
        "ADJ",
        "PART",
        "DET",
        "CCONJ",
        "PROPN",
        "PRON",
        "X",
        "X",
        "ADV",
        "INTJ",
        "VERB",
        "AUX",
    ]
    return names[upos_id]


def load_ud_yue():
    from datasets import load_dataset

    # Load the universal_dependencies dataset from Hugging Face
    dataset = load_dataset('universal-dependencies/universal_dependencies', 'yue_hk', trust_remote_code=True)

    # Gather all word segmented utterances
    utterances = [[(token, upos_id_to_str(pos)) for token, pos in zip(sentence['tokens'], sentence['upos'])] for sentence in dataset['test']]

    return utterances


def split_ud_yue():
    import random

    random.seed(42)

    utterances = load_ud_yue()

    # Shuffle the utterances to ensure randomness
    random.shuffle(utterances)

    # Sample 10 random utterances for in_context_samples
    in_context_samples = utterances[:10]

    # The rest of the utterances will be used as testing_samples
    testing_samples = utterances[10:]

    return in_context_samples


def generate_prompt_v1(prompt_dataset):
    if prompt_dataset == 'hkcancor':
        in_context_samples = split_hkcancor()
    elif prompt_dataset == 'ud_yue':
        in_context_samples = split_ud_yue()
    
    # Format in-context samples for the prompt
    in_context_prompt = "\n\n".join([
        f'EXAMPLE INPUT SENTENCE:\n{"".join([word for word, pos in sample])}\n\nEXAMPLE JSON OUTPUT:\n{json.dumps({"pos_tagged_words": [[word, pos] for word, pos in sample]}, ensure_ascii=False)}'
        for sample in in_context_samples
    ])

    # Update the word segmentation prompt with in-context samples
    pos_prompt = f"""You are an expert at Cantonese word segmentation and POS tagging. Output the segmented words with their parts of speech as JSON arrays. ALWAYS preserve typos, fragmented chunks, and punctuations in the original sentence. Output the parts of speech in the Universal Dependencies v2 tagset with 17 tags:
ADJ: adjective
ADP: adposition
ADV: adverb
AUX: auxiliary
CCONJ: coordinating conjunction
DET: determiner
INTJ: interjection
NOUN: noun
NUM: numeral
PART: particle
PRON: pronoun
PROPN: proper noun
PUNCT: punctuation
SCONJ: subordinating conjunction
SYM: symbol
VERB: verb
X: other

{in_context_prompt}"""
    
    return pos_prompt



def generate_prompt_v2(prompt_dataset):
    if prompt_dataset == 'hkcancor':
        in_context_samples = split_hkcancor()
    elif prompt_dataset == 'ud_yue':
        in_context_samples = split_ud_yue()
    
    # Format in-context samples for the prompt
    in_context_prompt = "\n\n".join([
        f'EXAMPLE INPUT SENTENCE:\n{"".join([word for word, pos in sample])}\n\nEXAMPLE JSON OUTPUT:\n{json.dumps({"pos_tagged_words": [[word, pos] for word, pos in sample]}, ensure_ascii=False)}'
        for sample in in_context_samples
    ])

    # Update the word segmentation prompt with in-context samples
    pos_prompt = f"""You are an expert at Cantonese word segmentation and POS tagging. Output the segmented words with their parts of speech as JSON arrays. ALWAYS preserve typos, fragmented chunks, and punctuations in the original sentence. Output the parts of speech in the Universal Dependencies v2 tagset with 17 tags:


ADJ: Adjectives are words that typically modify nouns and specify their properties or attributes. They may also function as predicates. These include the categories known as å€åˆ¥è© and å½¢å®¹è©.

å¥½/ADJ é¢¨æ™¯
å¤©æ°£ å¥½ å¥½/ADJ

The adjective may by accompanied by the particle å˜… when functioning as a prenominal modifier (for either å€åˆ¥è© or å½¢å®¹è©), and often obligatorily when functioning as a predicate if it is a å€åˆ¥è©.

åš´é‡/ADJ å˜… å•é¡Œ
å‘¢ ç¨® ç™Œç—‡ ä¿‚ æ…¢æ€§/ADJ å˜…

Note that ordinal numerals such as ç¬¬ä¸€ and ç¬¬ä¸‰ are to be treated as adjectives and tagged ADJ per UD specifications, even though they are traditionally classified as numerals in Chinese.

Examples
å½¢å®¹è©: å¥½, éš, ç´°, è€
å€åˆ¥è©: å¥³, æ…¢æ€§, è¦ªç”Ÿ
Ordinal numbers: ç¬¬ä¸€, ç¬¬ä¸‰, ç¬¬äº”åä¸‰


ADP: The Cantonese ADP covers three categories of function words analyzed as adpositions: (1) prepositions, (2) valence markers, and (3) â€œlocalizersâ€/postpositions.

Prepositions introduce an extra argument to the event/main verb in a clause or give information about the time or the location or direction, etc.

ä½¢ å–º/ADP 2000å¹´ å‡ºä¸–

Valence markers such as å°‡ and the formal è¢« are also tagged ADP.

ä½  å°‡/ADP å•² å˜¢ æ“º ä½ å–‡
å…‡æ‰‹ å‡Œæ™¨ ä¸€é» è¢«/ADP æ•

Localizers (also known as æ–¹ä½è©), typically indicate spatial information in relation to the noun preceding it. Some localizers have also grammaticalized into clausal markers indicating temporal information. Localizers with the clausal function are still tagged as ADP (but are labeled with the dependency relation mark).

æˆ‘ æ“º å’— å•² æ¢¨ å–º æ± åº¦/ADP
ä½¢ è€å©† æ­» å’— ä¹‹å¾Œ/ADP ï¼Œ ä½¢ éƒ½ å”” æƒ³ ç”Ÿå­˜ è½å» å˜‘

Examples
Prepositions: å–º, ç•€, ç”±, ç‚ºå’—
Valence markers: å°‡, è¢« (passive)
Localizers / postpositions: åº¦, ä¹‹å¾Œ, ä¹‹å‰, æ™‚


ADV:Adverbs (å‰¯è© / fu3ci4) typically modify verbs, adjectives, and other adverbs for such categories as time, manner, degree, frequency, or negation.

ä½  æ…¢æ…¢/ADV è¬›
ä½¢ éå¸¸ä¹‹/ADV é–‹å¿ƒ
ä½¢å“‹ å¥½/ADV æ—©/ADV èµ·èº«

Some adverbs also modify clauses with conjunctive and discursive functions.

ä½¢ åè€Œ/ADV å†‡ åŒ æˆ‘ è¬›
ä»Šæ—¥ ç•¶ç„¶/ADV å†‡ äºº å–‡

A small number of adverbs may also modify numerals and determiners, or nouns and pronouns.

å·®å””å¤š/ADV äº”åƒ
é€£/ADV ä½¢ éƒ½ å”” å» å˜‘

There is a closed subclass of pronominal adverbs that refer to circumstances in context, rather than naming them directly; similarly to pronouns, these can be categorized as interrogative, demonstrative, etc. These should be treated as adverbs when modifying a predicate, but otherwise some of them can function as a nominal in the syntax, in which case they should be tagged PRON.

ä½  é»è§£/ADV å”” ä¾†
æˆ‘ ä¿‚ å’æ¨£/ADV åš å˜…

Note that although some adverbs express temporal information, many common time expressions (e.g., ä»Šæ—¥, èˆŠå¹´, å¤œæ™š) are actually nouns and should be tagged NOUN.

Examples
Manner adverbs: æ…¢æ…¢, äº’ç›¸
Temporal, aspectual, and modal adverbs: å°±åšŸ, å–ºåº¦, å¯§å¯, å¯èƒ½ (NB: å¯èƒ½ can also function as ADJ â€œpossibleâ€ and NOUN â€œpossibilityâ€)
Conjunctive adverbs: æ‰€ä»¥, åè€Œ, ç„¶å¾Œ, å™‰, å°±
Frequency and duration adverbs: æˆæ—¥, ä¸€é™£
Negation adverbs: å””, æœª
Numeral-modifying adverbs: å¤§æ¦‚, å·®å””å¤š, è‡³å°‘, æœ€å¤š
Noun-modifying adverbs: é€£
Pronominal adverbs: é»æ¨£, å™‰æ¨£, å’, é»è§£
Other: éƒ½, äº¦éƒ½, å…ˆè‡³, è¶ŠåšŸè¶Š, ç•¶ç„¶, è­¬å¦‚, è­¬å¦‚è©±, ä¾‹å¦‚, å¥½ä¼¼ (note å¥½ä¼¼ can also function as a main verb; an example of the adverbial usage is å¥½ä¼¼ ä½¢ ç´æ—¥ å™‰ è¬›â€¦)


AUX: An auxiliary is a word that accompanies the lexical verb of a verb phrase and expresses grammatical distinctions not carried by the lexical verb, such as person, number, tense, mood, aspect, and voice.

In Cantonese, auxiliaries can be divided into modal and modal-like auxiliaries which are mostly preverbal (except for the postverbal usage of å¾—) and do not have to be adjacent to the verb, and aspect markers, which must come immediately after the verb and certain verb compounds. Note that some modal auxiliaries can also function as main verbs, usually when they have a direct object or full clausal complement.

Examples
Modal and modal-like auxiliaries: èƒ½å¤ , æœƒ, å¯ä»¥, æ‡‰è©², è‚¯, æ•¢, æœ‰ (perfective), å†‡ (negative perfective), å¾—
Aspect markers: å’— (perfective), ä½ (durative), é (experiential), ç·Š (progressive), å“ (delimitative), é–‹ (habitual)


CCONJ: A coordinating conjunction is a word that links words or larger constituents without syntactically subordinating one to the other and expresses a semantic relationship between them.

Examples
â€œandâ€: åŒ, åŒåŸ‹, è€Œ, è€Œä¸”. Note that åŒ may also function as a preposition (ADP)
â€œorâ€: æˆ–è€…, å®š(ä¿‚)
â€œbutâ€: ä½†ä¿‚


DET: Determiners are words that modify nouns or noun phrases and express the reference of the noun phrase in context.

Note that Chinese does not traditionally define determiners as a separate word class, but categorizes them as pronouns and/or adjectives. For this reason, in the UD framework some words in Cantonese may function as determiners and be tagged DET in one syntactic context (i.e., when modifying a noun), and as pronouns (tagged PRON) when behaving as a nominal or the head of a nominal phrase.

Examples
Demonstrative determiners: å‘¢, å—°
Possessive determiner: æœ¬
Quantifying determiners:
Definite: æ¯, æˆ, å…¨, å…¨éƒ¨, æ‰€æœ‰
Indefinite: ä»»ä½•, æœ‰å•², (å¥½)å¤š, (å¥½)å°‘, å¹¾, å¤§éƒ¨åˆ†, å””å°‘, å¤šæ•¸
Interrogative determiners: é‚Š, ä¹œ(å˜¢), å’©, å¹¾å¤š
Other: ä¸Š, ä¸‹, å‰, å¾Œ, é ­, å…¶é¤˜, æŸ, å…¶ä»–, å¦(å¤–), åŒ


INTJ: An interjection is a word that is used most often as an exclamation or part of an exclamation. It typically expresses an emotional reaction, is not syntactically related to other accompanying expressions, and may include a combination of sounds not otherwise found in the language.

Note that words primarily belonging to another part of speech retain their original category when used in exclamations. For example, ä¿‚ï¼ would still be tagged VERB.

Onomatopoeic words (æ“¬è²è©) should only be treated as interjections if they are used as an exclamation (e.g., å–µ!), otherwise they should be tagged according to their syntactic function in context (often as adverbs in Chinese, e.g., ä½¢å“‹ å±å±å–³å–³/ADV å™‰ å«).

Examples: å“¦, å“å‘€, å’¦


NOUN: Nouns are a part of speech typically denoting a person, place, thing, animal, or idea.

The NOUN tag is intended for common nouns only. See PROPN for proper nouns and PRON for pronouns.

As a special case, classifiers (é‡è©) are also tagged NOUN per UD guidelines. Their classifier status may be preserved in the feature column (FEATS) as NounType=CLf.

Examples
Nouns: æ¯, è‰, æ°§æ°£, åœ°æ–¹, èƒ½åŠ›, æ­·å²
Classifiers: å€‹, æ¢, æœ¬, å°, æ¯, ç£…, å¹´


NUM: A numeral is a word, functioning most typically as a determiner, a pronoun or an adjective, that expresses a number and a relation to the number, such as quantity, sequence, frequency or fraction.

Cardinal numerals are covered by NUM regardless of syntactic function and regardless of whether they are expressed as words (äº” / ng5 â€œfiveâ€) or digits (5). By contrast, ordinal numerals (such as ç¬¬ä¸€) are always tagged ADJ.

Examples: 1, 2, 3, 4, 5, 100, 10,358, 5.23, 3/4, ä¸€, äºŒ, ä¸‰, ä¸€ç™¾, äº”åå…­, ä¸€è¬ä¸‰ç™¾äº”åå…«, å››åˆ†ä¹‹ä¸‰


PART: Particles are function words that must be associated with another word, phrase, or clause to impart meaning and that do not satisfy definitions of other universal parts of speech (such as ADP, AUX, CCONJ, or SCONJ).

In Cantonese, particles include the genitive/associative/relativizer/nominalizer marker å˜…; å¾— and åˆ° in V-å¾—/åˆ° extent/descriptive constructions; the manner adverbializer å™‰; the â€œet ceteraâ€ marker ç­‰(ç­‰); sentence-final particles; the quantifiers åŸ‹ and æ™’; the adversative è¦ª; and so on.

Examples
Genitive çš„: ç‹— å˜…/PART å°¾å·´
Associative çš„: é–‹å¿ƒ å˜…/PART å°æœ‹å‹
Relativizer çš„: è·´ å–®è»Š å˜…/PART äºº
Nominalizer çš„: é£Ÿ å”” æ™’ å˜…/PART å”” å‡† èµ°
Extent/descriptive å¾—åˆ°:
ä½¢ è·‘ å¾—/PART å¥½ å¿«
ä½¢ è·‘ åˆ°/PART å‡º æ™’ æ±—
Adverbializer åœ°: ä½¢ æ…¢æ…¢ å™‰/PART è·‘
Etc. marker: å‘¢åº¦ æœ‰ æ¢¨ï¼Œæ©™ï¼Œç­‰ç­‰/PART
Sentence-final particles:
ä½  æƒ³ å» å‘€/PART ï¼Ÿ
æˆ‘å“‹ ä¸€é½Š å» å–‡/PART
ä½¢ ä¸€ å€‹ äºº å•«/PART
ä½¢å“‹ ä¸€å®š åšŸ å˜…/PART
Quantifiers:
ç•€ åŸ‹/PART æˆ‘
ä½¢å“‹ èµ° æ™’/PART
Adversative è¦ª: ä½¢ è·Œ è¦ª/PART


PRON: Pronouns are words that substitute for nouns or noun phrases, whose meaning is recoverable from the linguistic or extralinguistic context. Some pronouns â€“ in particular certain demonstrative, total, indefinite, and interrogatives pronouns â€“ may also function as determiners (DET) and are tagged as such when functioning as a modifier of a noun.

Examples
Personal pronouns: æˆ‘, ä½ , ä½¢, æˆ‘å“‹, ä½ å“‹, ä½¢å“‹, äººå“‹
Reciprocal and reflexive pronouns: è‡ªå·±, æˆ‘è‡ªå·±, ä½ è‡ªå·±, ä½¢å“‹è‡ªå·±
Total pronouns: å…¨éƒ¨, å¤§å®¶
Indefinite pronouns: æœ‰å•², å¥½å¤š, å¤§éƒ¨åˆ†, å””å°‘, å¤šæ•¸, å…¶ä»–
Locational pronouns: å‘¢åº¦, å‘¢é‚Š, å‘¢è™•, å—°åº¦, å—°é‚Š, å—°è™•
Manner pronouns: å’æ¨£ (also ADV)
Interrogative pronouns: é‚Šå€‹, ä¹œ(å˜¢), å’©, å¹¾å¤š, é‚Šåº¦, é»æ¨£ (also ADV)
Other:
X + æ–¹: å°æ–¹, é›™æ–¹
X + è€…: å‰è€…, å¾Œè€…, å…©è€…, ä¸‰è€…
å…¶é¤˜


PROPN: A proper noun is a noun (or nominal content word) that is the name (or part of the name) of a specific individual, place, or object. For institutional names that contain regular words belonging to other parts of speech such as nouns (e.g., å…¬å¸, å¤§å­¸, etc.), those words should be segmented as their own tokens and still tagged their native part of speech; only the proper nouns in such complex names should be tagged PROPN.

Examples
å­”å­/PROPN
äºæ´²/PROPN
å¨å¨/PROPN æœ¨æ å…¬å¸


PUNCT: Punctuation marks are character groups used to delimit linguistic units in printed text.

Punctuation is not taken to include logograms such as $, %, and Â§, which are instead tagged as SYM.

Examples
Period: ã€‚
Comma: ï¼Œ ã€
Quotation marks: â€˜ â€™ â€œ â€ã€Œ ã€ ã€ ã€
Title marks: ã€Š ã€‹


SCONJ: A subordinating conjunction is a conjunction that links constructions by making one of them a constituent of the other. The subordinating conjunction typically marks the incorporated constituent which has the status of a subordinate clause.

Subordinating conjunctions in Cantonese include all markers of subordinate clauses, including conditional clauses, purpose clauses, etc.

In paired clauses where both clauses are marked by a conjunctive word and the first is subordinate to the second, we treat the conjunctive word in the first clause as SCONJ, whereas the one in the second, main clause as an adverb (ADV) (e.g., é›–ç„¶/SCONJâ€¦ ä½†ä¿‚/ADVâ€¦).

Examples: å¦‚æœ, å˜…è©±, é›–ç„¶, å³ç®¡, ç„¡è«–
åšŸ: ä½  æ…³ åšŸ/SCONJ åš ä¹œ å‘€ ?


SYM: A symbol is a word-like entity that differs from ordinary words by form, function, or both.

Many symbols are or contain special non-alphanumeric, non-standard logographic characters, similarly to punctuation. What makes them different from punctuation is that they can be substituted by normal words. This involves all currency symbols, e.g. $ 75 is identical to ä¸ƒåäº” èšŠ.

Mathematical operators form another group of symbols.

Another group of symbols is emoticons and emoji.

Examples
ï¼„, ï¼…, Â§, Â©
+, âˆ’, Ã—, Ã·, =, <, >
:), ^_^, ğŸ˜
siu.ming@universal.org, http://universaldependencies.org/, 1-800-COMPANY


VERB: A verb is a member of the syntactic class of words that typically signal events and actions, can constitute a minimal predicate in a clause, and govern the number and types of other constituents which may occur in the clause.

Despite its use in copular constructions, ä¿‚ is tagged as a verb due to its other non-copular meanings and functions.

Examples: æ¸¸æ°´, è½, å‘¼å¸, é¾æ„, æ±ºå®š, æœ‰, ä¿‚


X: The tag X is used for words that for some reason cannot be assigned a real part-of-speech category. It should be used very restrictively.

A special usage of X is for cases of code-switching where it is not possible (or meaningful) to analyze the intervening language grammatically (and where the dependency relation foreign is typically used in the syntactic analysis). This usage does not extend to ordinary loan words which should be assigned a normal part-of-speech.

Example
ä½¢ çªç„¶ è©± ï¼šã€Œ lkjwe/X ã€


{in_context_prompt}"""
    
    return pos_prompt


if __name__ == "__main__":
    if args.prompt_version == 'v1':
        pos_prompt = generate_prompt_v1(args.prompt_dataset)
    elif args.prompt_version == 'v2':
        pos_prompt = generate_prompt_v2(args.prompt_dataset)

    # Write the updated word segmentation prompt to the file
    with open(f'data/pos_{args.prompt_dataset}_prompt_{args.prompt_version}.txt', 'w', encoding='utf-8') as f:
        f.write(pos_prompt)

    if args.eval_dataset == 'ud_yue':
        testing_samples = load_ud_yue()
    elif args.eval_dataset == 'hkcancor':
        testing_samples = load_hkcancor()
    
    random.seed(42)
    random.shuffle(testing_samples)
    testing_samples = testing_samples[:args.sample_size]

    with open(f'outputs/pos_{args.eval_dataset}_{args.model}_prompt_{args.prompt_version}.jsonl', 'w', encoding='utf-8') as file, open(f'outputs/pos_errors_{args.eval_dataset}_{args.model}_prompt_{args.prompt_version}.jsonl', 'w', encoding='utf-8') as error_file:
        lock = Lock()
        def process_sample(sample):
            input_sentence = "".join([word for word, pos in sample])
            pos_result = segment_words(input_sentence)
            if 'error' in pos_result:
                print(f"POS tagging failed for sentence: {input_sentence}")
                print(f"Error: {pos_result['error']}")
                error_result = {
                    "reference": sample,
                    "error": pos_result['error'],
                    "hypothesis": pos_result['result']
                }
                with lock:
                    error_file.write(json.dumps(error_result, ensure_ascii=False) + '\n')
                    error_file.flush()
                result = {
                    "reference": sample,
                    "hypothesis": [(char, "X") for char in list(input_sentence)]
                }
            else:
                result = {
                    "reference": sample,
                    "hypothesis": pos_result
                }
            with lock:
                file.write(json.dumps(result, ensure_ascii=False) + '\n')
                file.flush()
        with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
            list(tqdm(executor.map(process_sample, testing_samples), total=len(testing_samples)))
