from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
import time
import json
from threading import Lock
import random
import pycantonese
import re
import argparse

random.seed(42)

parser = argparse.ArgumentParser(description="POS Tagging with OpenAI API")
parser.add_argument('--model', type=str, choices=['deepseek-chat', 'deepseek-coder', 'gpt-4o', 'gpt-4o-mini', 'doubao-pro-32k', 'doubao-lite-32k', 'doubao-pro-4k', 'doubao-pro-4k-sample-200-step-1', 'doubao-pro-32k-sample-2500_global_step_22', 'qwen-max', 'qwen-plus', 'qwen-turbo'], required=True, help='Model to use for POS tagging')
parser.add_argument('--sample_size', type=int, default=100, help='Number of samples to test')
parser.add_argument('--prompt_version', type=str, choices=['v1', 'v2'], required=True, help='Prompt version to use for POS tagging')
parser.add_argument('--prompt_dataset', type=str, choices=['hkcancor', 'ud_yue'], required=True, help='Dataset to use for POS tagging')
parser.add_argument('--eval_dataset', type=str, choices=['hkcancor', 'ud_yue'], required=True, help='Dataset to use for evaluation')
parser.add_argument('--segmentation_given', type=bool, default=False, help='Whether to use given segmentation')
parser.add_argument('--maximize_diversity', type=bool, default=False, help='Whether to maximize in-context example diversity')
parser.add_argument('--to_simplified', type=bool, default=False, help='Whether to convert to simplified Chinese')
parser.add_argument('--max_workers', type=int, default=100, help='Maximum number of workers to use for parallel processing')
args = parser.parse_args()

if args.model.startswith('deepseek'):
    base_url = "https://api.deepseek.com"
    with open('deepseek_api_key.txt', 'r') as file:
        api_key = file.read().strip()
elif args.model.startswith('gpt-4o'):
    base_url = None
    with open('openai_api_key.txt', 'r') as file:
        api_key = file.read().strip()
elif args.model.startswith('doubao'):
    base_url = "https://ark.cn-beijing.volces.com/api/v3"
    with open('doubao_api_key.txt', 'r') as file:
        api_key = file.read().strip()
    if args.model == 'doubao-pro-32k':
        model_id = 'ep-20240806163752-ktwsg'
    elif args.model == 'doubao-lite-32k':
        model_id = 'ep-20240806163806-zpq5g'
    elif args.model == 'doubao-pro-4k':
        model_id = 'ep-20240807063847-gksxp'
    elif args.model == 'doubao-pro-4k-sample-200-step-1':
        model_id = 'ep-20240807062723-twtzz'
    elif args.model == 'doubao-pro-32k-sample-2500_global_step_22':
        model_id = 'ep-20240807090556-gw22s'
elif args.model.startswith('qwen'):
    base_url = "https://dashscope.aliyuncs.com/compatible-mode/v1"
    with open('qwen_api_key.txt', 'r') as file:
        api_key = file.read().strip()

client = OpenAI(api_key=api_key, base_url=base_url)

valid_pos_tags = {"ADJ", "ADP", "ADV", "AUX", "CCONJ", "DET", "INTJ", "NOUN", "NUM", "PART", "PRON", "PROPN", "PUNCT", "SCONJ", "SYM", "VERB", "X"}

def segment_words(pos_prompt, input_words):
    attempts = 0
    while True:
        try:
            response = client.chat.completions.create(
                model=model_id if args.model.startswith('doubao') else args.model,
                messages=[{
                            "role": "system",
                            "content": pos_prompt
                          },
                          {
                            "role": "user",
                            "content": " ".join(input_words)
                          }],
                response_format={
                    'type': 'json_object'
                },
                max_tokens=2000,
                temperature=0,
                stream=False
            )
            result = response.choices[0].message.content
            try:
                result_json = json.loads(result)
            except json.JSONDecodeError as e:
                raise Exception(f"Failed to parse JSON response: {str(e)}")
            if "pos_tagged_words" in result_json and isinstance(result_json["pos_tagged_words"], list) and \
                all(isinstance(item, list) and len(item) == 2 and (all(isinstance(sub_item, str) for sub_item in item)) for item in result_json["pos_tagged_words"]):
                concatenated_words = "".join([word for word, pos in result_json["pos_tagged_words"]])
                if concatenated_words == "".join(input_words):
                    for word, pos in result_json["pos_tagged_words"]:
                        if pos not in valid_pos_tags:
                            raise Exception(f"Invalid POS tag '{pos}' in the result")
                    return result_json["pos_tagged_words"]
                else:
                    raise Exception(f"Segmentation result does not match the input sentence")
            else:
                raise Exception(f"Invalid segmentation result format")
        except Exception as e:
            time.sleep(1)
            attempts += 1
            if attempts >= 3:
                return {'error': str(e), 'result': result}


def load_hkcancor():
    # Load HKCanCor data
    hkcancor_data = pycantonese.hkcancor()

    # Gather all word segmented utterances
    utterances = [[(token.word, pycantonese.pos_tagging.hkcancor_to_ud(token.pos)) for token in utterance] for utterance in hkcancor_data.tokens(by_utterances=True)]

    return utterances


def split_hkcancor():
    random.seed(42)

    utterances = load_hkcancor()

    latin_fragmented_utterances = []
    latin_complete_utterances = []
    non_latin_utterances = []

    for utterance in utterances:
        if any(re.search(r'[a-zA-Z0-9]', char) for (word, pos) in utterance for char in word):
            if ''.join(word for word, pos in utterance).count('"') % 2 != 0:
                latin_fragmented_utterances.append(utterance)
            else:
                latin_complete_utterances.append(utterance)
        else:
            non_latin_utterances.append(utterance)

    # Sample 3 random latin_complete_utterances, 3 random latin_fragmented_utterances, and 4 random non_latin_utterances for in_context_samples
    random.shuffle(latin_complete_utterances)
    random.shuffle(latin_fragmented_utterances)
    random.shuffle(non_latin_utterances)
    in_context_samples = latin_complete_utterances[:3] + latin_fragmented_utterances[:3] + non_latin_utterances[:4]
    random.shuffle(in_context_samples)
    
    return in_context_samples


def upos_id_to_str(upos_id):
    names=[
        "NOUN",
        "PUNCT",
        "ADP",
        "NUM",
        "SYM",
        "SCONJ",
        "ADJ",
        "PART",
        "DET",
        "CCONJ",
        "PROPN",
        "PRON",
        "X",
        "X",
        "ADV",
        "INTJ",
        "VERB",
        "AUX",
    ]
    return names[upos_id]


def load_ud_yue():
    from datasets import load_dataset

    # Load the universal_dependencies dataset from Hugging Face
    dataset = load_dataset('universal-dependencies/universal_dependencies', 'yue_hk', trust_remote_code=True)

    # Gather all word segmented utterances
    utterances = [[(token, upos_id_to_str(pos)) for token, pos in zip(sentence['tokens'], sentence['upos'])] for sentence in dataset['test']]

    return utterances


def split_ud_yue(maximize_diversity=False):
    import random

    random.seed(42)

    utterances = load_ud_yue()

    # Shuffle the utterances to ensure randomness
    random.shuffle(utterances)

    if maximize_diversity:
        import math
        from collections import Counter

        # Count the frequency of each tag in the utterances
        tag_counter = Counter(pos for utterance in utterances for _, pos in utterance)

        # Calculate the weight for each tag inversely to its frequency
        tag_weights = {tag: 1 / math.log(count) for tag, count in tag_counter.items()}
        from collections import defaultdict

        # Initialize a frequency counter for seen (token, pos) pairs
        seen_pairs = defaultdict(int)

        in_context_samples = []
        while len(in_context_samples) < 10 and utterances:
            # Calculate diversity score for each utterance
            diversity_scores = []
            for utterance in utterances:
                score = 0
                for _, pos in utterance:
                    score += tag_weights[pos] / (1 + seen_pairs[(pos, _)])
                diversity_scores.append((utterance, score / math.sqrt(len(utterance))))

            # Sort utterances by diversity score in descending order
            sorted_utterances = sorted(diversity_scores, key=lambda x: x[1], reverse=True)

            # Select the utterance with the highest diversity score
            selected_utterance, _ = sorted_utterances[0]
            in_context_samples.append(selected_utterance)

            # Update the frequency counter for seen (token, pos) pairs
            for token, pos in selected_utterance:
                seen_pairs[(pos, token)] += 1

            # Remove the selected utterance from the list
            utterances.remove(selected_utterance)
    else:
        # Sample 10 random utterances for in_context_samples
        in_context_samples = utterances[:10]

    return in_context_samples


def generate_prompt_v1(prompt_dataset, segmentation_given, maximize_diversity=False):
    if prompt_dataset == 'hkcancor':
        in_context_samples = split_hkcancor()
    elif prompt_dataset == 'ud_yue':
        in_context_samples = split_ud_yue(maximize_diversity)
    
    # Format in-context samples for the prompt
    in_context_prompt = "\n\n".join([
        f'EXAMPLE INPUT SENTENCE:\n{(" " if segmentation_given else "").join([word for word, pos in sample])}\n\nEXAMPLE JSON OUTPUT:\n{json.dumps({"pos_tagged_words": [[word, pos] for word, pos in sample]}, ensure_ascii=False)}'
        for sample in in_context_samples
    ])

    # Update the word segmentation prompt with in-context samples
    pos_prompt = f"""You are an expert at Cantonese word segmentation and POS tagging. Output the segmented words with their parts of speech as JSON arrays. ALWAYS preserve typos, fragmented chunks, and punctuations in the original sentence. Output the parts of speech in the Universal Dependencies v2 tagset with 17 tags:
ADJ: adjective
ADP: adposition
ADV: adverb
AUX: auxiliary
CCONJ: coordinating conjunction
DET: determiner
INTJ: interjection
NOUN: noun
NUM: numeral
PART: particle
PRON: pronoun
PROPN: proper noun
PUNCT: punctuation
SCONJ: subordinating conjunction
SYM: symbol
VERB: verb
X: other

{in_context_prompt}"""
    
    return pos_prompt



def generate_prompt_v2(prompt_dataset, segmentation_given, maximize_diversity=False):
    if prompt_dataset == 'hkcancor':
        in_context_samples = split_hkcancor()
    elif prompt_dataset == 'ud_yue':
        in_context_samples = split_ud_yue(maximize_diversity)
    
    # Format in-context samples for the prompt
    in_context_prompt = "\n\n".join([
        f'EXAMPLE INPUT SENTENCE:\n{(" " if segmentation_given else "").join([word for word, pos in sample])}\n\nEXAMPLE JSON OUTPUT:\n{json.dumps({"pos_tagged_words": [[word, pos] for word, pos in sample]}, ensure_ascii=False)}'
        for sample in in_context_samples
    ])

    # Update the word segmentation prompt with in-context samples
    pos_prompt = f"""You are an expert at Cantonese word segmentation and POS tagging. Output the segmented words with their parts of speech as JSON arrays. ALWAYS preserve typos, fragmented chunks, and punctuations in the original sentence. Output the parts of speech in the Universal Dependencies v2 tagset with 17 tags:


ADJ: Adjectives are words that typically modify nouns and specify their properties or attributes. They may also function as predicates. These include the categories known as ÂçÄÂà•Ë©û and ÂΩ¢ÂÆπË©û.

Â•Ω/ADJ È¢®ÊôØ
Â§©Ê∞£ Â•Ω Â•Ω/ADJ

The adjective may by accompanied by the particle ÂòÖ when functioning as a prenominal modifier (for either ÂçÄÂà•Ë©û or ÂΩ¢ÂÆπË©û), and often obligatorily when functioning as a predicate if it is a ÂçÄÂà•Ë©û.

Âö¥Èáç/ADJ ÂòÖ ÂïèÈ°å
Âë¢ Á®Æ ÁôåÁóá ‰øÇ ÊÖ¢ÊÄß/ADJ ÂòÖ

Note that ordinal numerals such as Á¨¨‰∏Ä and Á¨¨‰∏â are to be treated as adjectives and tagged ADJ per UD specifications, even though they are traditionally classified as numerals in Chinese.

Examples
ÂΩ¢ÂÆπË©û: Â•Ω, Èùö, Á¥∞, ËÄÅ
ÂçÄÂà•Ë©û: Â•≥, ÊÖ¢ÊÄß, Ë¶™Áîü
Ordinal numbers: Á¨¨‰∏Ä, Á¨¨‰∏â, Á¨¨‰∫îÂçÅ‰∏â


ADP: The Cantonese ADP covers three categories of function words analyzed as adpositions: (1) prepositions, (2) valence markers, and (3) ‚Äúlocalizers‚Äù/postpositions.

Prepositions introduce an extra argument to the event/main verb in a clause or give information about the time or the location or direction, etc.

‰Ω¢ Âñ∫/ADP 2000Âπ¥ Âá∫‰∏ñ

Valence markers such as Â∞á and the formal Ë¢´ are also tagged ADP.

‰Ω† Â∞á/ADP Âï≤ Âò¢ Êì∫ ‰Ωé Âñá
ÂÖáÊâã ÂáåÊô® ‰∏ÄÈªû Ë¢´/ADP Êçï

Localizers (also known as Êñπ‰ΩçË©û), typically indicate spatial information in relation to the noun preceding it. Some localizers have also grammaticalized into clausal markers indicating temporal information. Localizers with the clausal function are still tagged as ADP (but are labeled with the dependency relation mark).

Êàë Êì∫ Âíó Âï≤ Ê¢® Âñ∫ Êû± Â∫¶/ADP
‰Ω¢ ËÄÅÂ©Ü Ê≠ª Âíó ‰πãÂæå/ADP Ôºå ‰Ω¢ ÈÉΩ Âîî ÊÉ≥ ÁîüÂ≠ò ËêΩÂéª Âòë

Examples
Prepositions: Âñ∫, ÁïÄ, Áî±, ÁÇ∫Âíó
Valence markers: Â∞á, Ë¢´ (passive)
Localizers / postpositions: Â∫¶, ‰πãÂæå, ‰πãÂâç, ÊôÇ


ADV:Adverbs (ÂâØË©û / fu3ci4) typically modify verbs, adjectives, and other adverbs for such categories as time, manner, degree, frequency, or negation.

‰Ω† ÊÖ¢ÊÖ¢/ADV Ë¨õ
‰Ω¢ ÈùûÂ∏∏‰πã/ADV ÈñãÂøÉ
‰Ω¢Âìã Â•Ω/ADV Êó©/ADV Ëµ∑Ë∫´

Some adverbs also modify clauses with conjunctive and discursive functions.

‰Ω¢ ÂèçËÄå/ADV ÂÜá Âêå Êàë Ë¨õ
‰ªäÊó• Áï∂ÁÑ∂/ADV ÂÜá ‰∫∫ Âñá

A small number of adverbs may also modify numerals and determiners, or nouns and pronouns.

Â∑ÆÂîîÂ§ö/ADV ‰∫îÂçÉ
ÈÄ£/ADV ‰Ω¢ ÈÉΩ Âîî Âéª Âòë

There is a closed subclass of pronominal adverbs that refer to circumstances in context, rather than naming them directly; similarly to pronouns, these can be categorized as interrogative, demonstrative, etc. These should be treated as adverbs when modifying a predicate, but otherwise some of them can function as a nominal in the syntax, in which case they should be tagged PRON.

‰Ω† ÈªûËß£/ADV Âîî ‰æÜ
Êàë ‰øÇ ÂíÅÊ®£/ADV ÂÅö ÂòÖ

Note that although some adverbs express temporal information, many common time expressions (e.g., ‰ªäÊó•, ËàäÂπ¥, Â§úÊôö) are actually nouns and should be tagged NOUN.

Examples
Manner adverbs: ÊÖ¢ÊÖ¢, ‰∫íÁõ∏
Temporal, aspectual, and modal adverbs: Â∞±Âöü, Âñ∫Â∫¶, ÂØßÂèØ, ÂèØËÉΩ (NB: ÂèØËÉΩ can also function as ADJ ‚Äúpossible‚Äù and NOUN ‚Äúpossibility‚Äù)
Conjunctive adverbs: ÊâÄ‰ª•, ÂèçËÄå, ÁÑ∂Âæå, Âôâ, Â∞±
Frequency and duration adverbs: ÊàêÊó•, ‰∏ÄÈô£
Negation adverbs: Âîî, Êú™
Numeral-modifying adverbs: Â§ßÊ¶Ç, Â∑ÆÂîîÂ§ö, Ëá≥Â∞ë, ÊúÄÂ§ö
Noun-modifying adverbs: ÈÄ£
Pronominal adverbs: ÈªûÊ®£, ÂôâÊ®£, ÂíÅ, ÈªûËß£
Other: ÈÉΩ, ‰∫¶ÈÉΩ, ÂÖàËá≥, Ë∂äÂöüË∂ä, Áï∂ÁÑ∂, Ë≠¨Â¶Ç, Ë≠¨Â¶ÇË©±, ‰æãÂ¶Ç, Â•Ω‰ºº (note Â•Ω‰ºº can also function as a main verb; an example of the adverbial usage is Â•Ω‰ºº ‰Ω¢ Áê¥Êó• Âôâ Ë¨õ‚Ä¶)


AUX: An auxiliary is a word that accompanies the lexical verb of a verb phrase and expresses grammatical distinctions not carried by the lexical verb, such as person, number, tense, mood, aspect, and voice.

In Cantonese, auxiliaries can be divided into modal and modal-like auxiliaries which are mostly preverbal (except for the postverbal usage of Âæó) and do not have to be adjacent to the verb, and aspect markers, which must come immediately after the verb and certain verb compounds. Note that some modal auxiliaries can also function as main verbs, usually when they have a direct object or full clausal complement.

Examples
Modal and modal-like auxiliaries: ËÉΩÂ§†, ÊúÉ, ÂèØ‰ª•, ÊáâË©≤, ËÇØ, Êï¢, Êúâ (perfective), ÂÜá (negative perfective), Âæó
Aspect markers: Âíó (perfective), ‰Ωè (durative), ÈÅé (experiential), Á∑ä (progressive), Âêì (delimitative), Èñã (habitual)


CCONJ: A coordinating conjunction is a word that links words or larger constituents without syntactically subordinating one to the other and expresses a semantic relationship between them.

Examples
‚Äúand‚Äù: Âêå, ÂêåÂüã, ËÄå, ËÄå‰∏î. Note that Âêå may also function as a preposition (ADP)
‚Äúor‚Äù: ÊàñËÄÖ, ÂÆö(‰øÇ)
‚Äúbut‚Äù: ‰ΩÜ‰øÇ


DET: Determiners are words that modify nouns or noun phrases and express the reference of the noun phrase in context.

Note that Chinese does not traditionally define determiners as a separate word class, but categorizes them as pronouns and/or adjectives. For this reason, in the UD framework some words in Cantonese may function as determiners and be tagged DET in one syntactic context (i.e., when modifying a noun), and as pronouns (tagged PRON) when behaving as a nominal or the head of a nominal phrase.

Examples
Demonstrative determiners: Âë¢, Âó∞
Possessive determiner: Êú¨
Quantifying determiners:
Definite: ÊØè, Êàê, ÂÖ®, ÂÖ®ÈÉ®, ÊâÄÊúâ
Indefinite: ‰ªª‰Ωï, ÊúâÂï≤, (Â•Ω)Â§ö, (Â•Ω)Â∞ë, Âπæ, Â§ßÈÉ®ÂàÜ, ÂîîÂ∞ë, Â§öÊï∏
Interrogative determiners: ÈÇä, ‰πú(Âò¢), Âí©, ÂπæÂ§ö
Other: ‰∏ä, ‰∏ã, Ââç, Âæå, È†≠, ÂÖ∂È§ò, Êüê, ÂÖ∂‰ªñ, Âè¶(Â§ñ), Âêå


INTJ: An interjection is a word that is used most often as an exclamation or part of an exclamation. It typically expresses an emotional reaction, is not syntactically related to other accompanying expressions, and may include a combination of sounds not otherwise found in the language.

Note that words primarily belonging to another part of speech retain their original category when used in exclamations. For example, ‰øÇÔºÅ would still be tagged VERB.

Onomatopoeic words (Êì¨ËÅ≤Ë©û) should only be treated as interjections if they are used as an exclamation (e.g., Âñµ!), otherwise they should be tagged according to their syntactic function in context (often as adverbs in Chinese, e.g., ‰Ω¢Âìã Âê±Âê±Âñ≥Âñ≥/ADV Âôâ Âè´).

Examples: Âì¶, ÂìéÂëÄ, Âí¶


NOUN: Nouns are a part of speech typically denoting a person, place, thing, animal, or idea.

The NOUN tag is intended for common nouns only. See PROPN for proper nouns and PRON for pronouns.

As a special case, classifiers (ÈáèË©û) are also tagged NOUN per UD guidelines. Their classifier status may be preserved in the feature column (FEATS) as NounType=CLf.

Examples
Nouns: ÊùØ, Ëçâ, Ê∞ßÊ∞£, Âú∞Êñπ, ËÉΩÂäõ, Ê≠∑Âè≤
Classifiers: ÂÄã, Ê¢ù, Êú¨, Â∞ç, ÊùØ, Á£Ö, Âπ¥


NUM: A numeral is a word, functioning most typically as a determiner, a pronoun or an adjective, that expresses a number and a relation to the number, such as quantity, sequence, frequency or fraction.

Cardinal numerals are covered by NUM regardless of syntactic function and regardless of whether they are expressed as words (‰∫î / ng5 ‚Äúfive‚Äù) or digits (5). By contrast, ordinal numerals (such as Á¨¨‰∏Ä) are always tagged ADJ.

Examples: 1, 2, 3, 4, 5, 100, 10,358, 5.23, 3/4, ‰∏Ä, ‰∫å, ‰∏â, ‰∏ÄÁôæ, ‰∫îÂçÅÂÖ≠, ‰∏ÄËê¨‰∏âÁôæ‰∫îÂçÅÂÖ´, ÂõõÂàÜ‰πã‰∏â


PART: Particles are function words that must be associated with another word, phrase, or clause to impart meaning and that do not satisfy definitions of other universal parts of speech (such as ADP, AUX, CCONJ, or SCONJ).

In Cantonese, particles include the genitive/associative/relativizer/nominalizer marker ÂòÖ; Âæó and Âà∞ in V-Âæó/Âà∞ extent/descriptive constructions; the manner adverbializer Âôâ; the ‚Äúet cetera‚Äù marker Á≠â(Á≠â); sentence-final particles; the quantifiers Âüã and Êôí; the adversative Ë¶™; and so on.

Examples
Genitive ÁöÑ: Áãó ÂòÖ/PART Â∞æÂ∑¥
Associative ÁöÑ: ÈñãÂøÉ ÂòÖ/PART Â∞èÊúãÂèã
Relativizer ÁöÑ: Ë∑¥ ÂñÆËªä ÂòÖ/PART ‰∫∫
Nominalizer ÁöÑ: È£ü Âîî Êôí ÂòÖ/PART Âîî ÂáÜ Ëµ∞
Extent/descriptive ÂæóÂà∞:
‰Ω¢ Ë∑ë Âæó/PART Â•Ω Âø´
‰Ω¢ Ë∑ë Âà∞/PART Âá∫ Êôí Ê±ó
Adverbializer Âú∞: ‰Ω¢ ÊÖ¢ÊÖ¢ Âôâ/PART Ë∑ë
Etc. marker: Âë¢Â∫¶ Êúâ Ê¢®ÔºåÊ©ôÔºåÁ≠âÁ≠â/PART
Sentence-final particles:
‰Ω† ÊÉ≥ Âéª ÂëÄ/PART Ôºü
ÊàëÂìã ‰∏ÄÈΩä Âéª Âñá/PART
‰Ω¢ ‰∏Ä ÂÄã ‰∫∫ Âï´/PART
‰Ω¢Âìã ‰∏ÄÂÆö Âöü ÂòÖ/PART
Quantifiers:
ÁïÄ Âüã/PART Êàë
‰Ω¢Âìã Ëµ∞ Êôí/PART
Adversative Ë¶™: ‰Ω¢ Ë∑å Ë¶™/PART


PRON: Pronouns are words that substitute for nouns or noun phrases, whose meaning is recoverable from the linguistic or extralinguistic context. Some pronouns ‚Äì in particular certain demonstrative, total, indefinite, and interrogatives pronouns ‚Äì may also function as determiners (DET) and are tagged as such when functioning as a modifier of a noun.

Examples
Personal pronouns: Êàë, ‰Ω†, ‰Ω¢, ÊàëÂìã, ‰Ω†Âìã, ‰Ω¢Âìã, ‰∫∫Âìã
Reciprocal and reflexive pronouns: Ëá™Â∑±, ÊàëËá™Â∑±, ‰Ω†Ëá™Â∑±, ‰Ω¢ÂìãËá™Â∑±
Total pronouns: ÂÖ®ÈÉ®, Â§ßÂÆ∂
Indefinite pronouns: ÊúâÂï≤, Â•ΩÂ§ö, Â§ßÈÉ®ÂàÜ, ÂîîÂ∞ë, Â§öÊï∏, ÂÖ∂‰ªñ
Locational pronouns: Âë¢Â∫¶, Âë¢ÈÇä, Âë¢Ëôï, Âó∞Â∫¶, Âó∞ÈÇä, Âó∞Ëôï
Manner pronouns: ÂíÅÊ®£ (also ADV)
Interrogative pronouns: ÈÇäÂÄã, ‰πú(Âò¢), Âí©, ÂπæÂ§ö, ÈÇäÂ∫¶, ÈªûÊ®£ (also ADV)
Other:
X + Êñπ: Â∞çÊñπ, ÈõôÊñπ
X + ËÄÖ: ÂâçËÄÖ, ÂæåËÄÖ, ÂÖ©ËÄÖ, ‰∏âËÄÖ
ÂÖ∂È§ò


PROPN: A proper noun is a noun (or nominal content word) that is the name (or part of the name) of a specific individual, place, or object. For institutional names that contain regular words belonging to other parts of speech such as nouns (e.g., ÂÖ¨Âè∏, Â§ßÂ≠∏, etc.), those words should be segmented as their own tokens and still tagged their native part of speech; only the proper nouns in such complex names should be tagged PROPN.

Examples
Â≠îÂ≠ê/PROPN
‰∫ûÊ¥≤/PROPN
Â®ÅÂ®Å/PROPN Êú®Êùê ÂÖ¨Âè∏


PUNCT: Punctuation marks are character groups used to delimit linguistic units in printed text.

Punctuation is not taken to include logograms such as $, %, and ¬ß, which are instead tagged as SYM.

Examples
Period: „ÄÇ
Comma: Ôºå „ÄÅ
Quotation marks: ‚Äò ‚Äô ‚Äú ‚Äù„Äå „Äç „Äé „Äè
Title marks: „Ää „Äã


SCONJ: A subordinating conjunction is a conjunction that links constructions by making one of them a constituent of the other. The subordinating conjunction typically marks the incorporated constituent which has the status of a subordinate clause.

Subordinating conjunctions in Cantonese include all markers of subordinate clauses, including conditional clauses, purpose clauses, etc.

In paired clauses where both clauses are marked by a conjunctive word and the first is subordinate to the second, we treat the conjunctive word in the first clause as SCONJ, whereas the one in the second, main clause as an adverb (ADV) (e.g., ÈõñÁÑ∂/SCONJ‚Ä¶ ‰ΩÜ‰øÇ/ADV‚Ä¶).

Examples: Â¶ÇÊûú, ÂòÖË©±, ÈõñÁÑ∂, Âç≥ÁÆ°, ÁÑ°Ë´ñ
Âöü: ‰Ω† ÊÖ≥ Âöü/SCONJ ÂÅö ‰πú ÂëÄ ?


SYM: A symbol is a word-like entity that differs from ordinary words by form, function, or both.

Many symbols are or contain special non-alphanumeric, non-standard logographic characters, similarly to punctuation. What makes them different from punctuation is that they can be substituted by normal words. This involves all currency symbols, e.g. $ 75 is identical to ‰∏ÉÂçÅ‰∫î Ëöä.

Mathematical operators form another group of symbols.

Another group of symbols is emoticons and emoji.

Examples
ÔºÑ, ÔºÖ, ¬ß, ¬©
+, ‚àí, √ó, √∑, =, <, >
:), ^_^, üòù
siu.ming@universal.org, http://universaldependencies.org/, 1-800-COMPANY


VERB: A verb is a member of the syntactic class of words that typically signal events and actions, can constitute a minimal predicate in a clause, and govern the number and types of other constituents which may occur in the clause.

Despite its use in copular constructions, ‰øÇ is tagged as a verb due to its other non-copular meanings and functions.

Examples: Ê∏∏Ê∞¥, ËÅΩ, ÂëºÂê∏, ÈçæÊÑè, Ê±∫ÂÆö, Êúâ, ‰øÇ


X: The tag X is used for words that for some reason cannot be assigned a real part-of-speech category. It should be used very restrictively.

A special usage of X is for cases of code-switching where it is not possible (or meaningful) to analyze the intervening language grammatically (and where the dependency relation foreign is typically used in the syntactic analysis). This usage does not extend to ordinary loan words which should be assigned a normal part-of-speech.

Example
‰Ω¢ Á™ÅÁÑ∂ Ë©± Ôºö„Äå lkjwe/X „Äç


{in_context_prompt}"""
    
    return pos_prompt

# Load the traditional to simplified character mappings
t2s_map = {}
with open('data/t2s.txt', 'r', encoding='utf-8') as f:
    for line in f:
        trad, simp = line.strip().split()
        t2s_map[trad] = simp

def to_simplified(text):
    # Convert the text from traditional to simplified characters
    simplified_text = ''.join(t2s_map.get(char, char) for char in text)
    return simplified_text


if __name__ == "__main__":
    if args.prompt_version == 'v1':
        pos_prompt = generate_prompt_v1(args.prompt_dataset, args.segmentation_given, args.maximize_diversity)
    elif args.prompt_version == 'v2':
        pos_prompt = generate_prompt_v2(args.prompt_dataset, args.segmentation_given, args.maximize_diversity)

    # Write the updated word segmentation prompt to the file
    with open(f'data/pos_{args.prompt_dataset}_prompt_{args.prompt_version}{"_max_diversity" if args.maximize_diversity else ""}{"_simplified" if args.to_simplified else ""}{f"_prompt_dataset_{args.prompt_dataset}" if args.prompt_dataset != args.eval_dataset else ""}{"_segmentation_given" if args.segmentation_given else ""}.txt', 'w', encoding='utf-8') as f:
        f.write(pos_prompt)

    if args.to_simplified:
        pos_prompt = to_simplified(pos_prompt)

    if args.eval_dataset == 'ud_yue':
        testing_samples = load_ud_yue()
    elif args.eval_dataset == 'hkcancor':
        testing_samples = load_hkcancor()
    
    random.seed(42)
    random.shuffle(testing_samples)
    testing_samples = testing_samples[:args.sample_size]

    with open(f'outputs/pos_{args.eval_dataset}_{args.model}_prompt_{args.prompt_version}{"_max_diversity" if args.maximize_diversity else ""}{"_simplified" if args.to_simplified else ""}{f"_prompt_dataset_{args.prompt_dataset}" if args.prompt_dataset != args.eval_dataset else ""}{"_segmentation_given" if args.segmentation_given else ""}.jsonl', 'w', encoding='utf-8') as file, open(f'outputs/pos_errors_{args.eval_dataset}_{args.model}_prompt_{args.prompt_version}{"_max_diversity" if args.maximize_diversity else ""}{"_simplified" if args.to_simplified else ""}{f"_prompt_dataset_{args.prompt_dataset}" if args.prompt_dataset != args.eval_dataset else ""}{"_segmentation_given" if args.segmentation_given else ""}.jsonl', 'w', encoding='utf-8') as error_file:
        lock = Lock()
        def process_sample(sample):
            sample = [(to_simplified(word), pos) for word, pos in sample] if args.to_simplified else sample
            input_words = [word for word, pos in sample]
            input_sentence = "".join(input_words)
            pos_result = segment_words(pos_prompt, input_words if args.segmentation_given else [input_sentence])
            if 'error' in pos_result:
                print(f"POS tagging failed for sentence: {input_sentence}")
                print(f"Error: {pos_result['error']}")
                error_result = {
                    "reference": sample,
                    "error": pos_result['error'],
                    "hypothesis": pos_result['result']
                }
                with lock:
                    error_file.write(json.dumps(error_result, ensure_ascii=False) + '\n')
                    error_file.flush()
                result = {
                    "reference": sample,
                    "hypothesis": [(char, "X") for char in list("".join([word for word, pos in sample]))]
                }
            else:
                result = {
                    "reference": sample,
                    "hypothesis": pos_result
                }
            with lock:
                file.write(json.dumps(result, ensure_ascii=False) + '\n')
                file.flush()
        with ThreadPoolExecutor(max_workers=args.max_workers) as executor:
            list(tqdm(executor.map(process_sample, testing_samples), total=len(testing_samples)))
